#!/usr/bin/env python3
"""
Runner principal para tests de precisiÃ³n (accuracy) de detecciÃ³n de gestos.
Ejecuta tests de accuracy para todos los controladores y genera reportes detallados.
"""

import sys
import os
import time
import unittest
import importlib
from datetime import datetime
from typing import Dict, List, Any
import json

# Agregar el directorio raÃ­z al path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../../')))

# Importar tests de accuracy
from tests.performance.metrics.accuracy.test_mouse_controller_accuracy import TestMouseControllerAccuracy
from tests.performance.metrics.accuracy.test_volume_controller_accuracy import TestVolumeControllerAccuracy
from tests.performance.metrics.accuracy.test_navigation_controller_accuracy import TestNavigationControllerAccuracy
from tests.performance.metrics.accuracy.test_system_controller_accuracy import TestSystemControllerAccuracy
from tests.performance.metrics.accuracy.test_shortcuts_controller_accuracy import TestShortcutsControllerAccuracy
from tests.performance.metrics.accuracy.test_multimedia_controller_accuracy import TestMultimediaControllerAccuracy
from tests.performance.metrics.accuracy.test_app_controller_accuracy import TestAppControllerAccuracy

class AccuracyTestRunner:
    """Runner para tests de precisiÃ³n de gestos"""
    
    def __init__(self):
        self.results = {}
        self.test_start_time = None
        self.test_end_time = None
        
        # Mapeo de tests disponibles
        self.available_tests = {
            'mouse': TestMouseControllerAccuracy,
            'volume': TestVolumeControllerAccuracy,
            'navigation': TestNavigationControllerAccuracy,
            'system': TestSystemControllerAccuracy,
            'shortcuts': TestShortcutsControllerAccuracy,
            'multimedia': TestMultimediaControllerAccuracy,
            'app': TestAppControllerAccuracy
        }
        
        # Objetivos de precisiÃ³n por controlador
        self.accuracy_targets = {
            'mouse': 0.94,        # 94% - Muy alto para interacciÃ³n crÃ­tica
            'volume': 0.92,       # 92% - Alto para controles de audio
            'navigation': 0.91,   # 91% - Alto para navegaciÃ³n frecuente
            'shortcuts': 0.93,    # 93% - Muy alto para atajos crÃ­ticos
            'multimedia': 0.89,   # 89% - Bueno para controles multimedia
            'system': 0.88,       # 88% - Conservador para operaciones crÃ­ticas
            'app': 0.86          # 86% - Moderado para gestiÃ³n de apps
        }
    
    def run_controller_tests(self, controller_name: str) -> Dict[str, Any]:
        """
        Ejecuta tests de accuracy para un controlador especÃ­fico
        
        Args:
            controller_name: Nombre del controlador a testear
            
        Returns:
            Dict con resultados del test
        """
        if controller_name not in self.available_tests:
            raise ValueError(f"Controlador {controller_name} no disponible")
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ TESTING ACCURACY: {controller_name.upper()} CONTROLLER")
        print(f"{'='*60}")
        
        test_class = self.available_tests[controller_name]
        target_accuracy = self.accuracy_targets[controller_name]
        
        # Crear suite de tests
        suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
        
        # Capturar resultados
        stream = TestResultCapture()
        runner = unittest.TextTestRunner(stream=stream, verbosity=2)
        
        start_time = time.time()
        result = runner.run(suite)
        end_time = time.time()
        
        # Procesar resultados
        controller_results = {
            'controller': controller_name,
            'target_accuracy': target_accuracy,
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'success_rate': (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0,
            'execution_time': end_time - start_time,
            'timestamp': datetime.now().isoformat(),
            'test_details': {
                'failures': [str(f[0]) + ": " + str(f[1]) for f in result.failures],
                'errors': [str(e[0]) + ": " + str(e[1]) for e in result.errors]
            }
        }
        
        # Determinar estado del test
        if controller_results['success_rate'] >= 0.95:
            status = "âœ… EXCELENTE"
        elif controller_results['success_rate'] >= 0.85:
            status = "âœ… BUENO"
        elif controller_results['success_rate'] >= 0.70:
            status = "âš ï¸ ACEPTABLE"
        else:
            status = "âŒ NECESITA MEJORAS"
        
        controller_results['status'] = status
        
        # Mostrar resumen
        print(f"\nğŸ“Š RESUMEN {controller_name.upper()}:")
        print(f"   ğŸ¯ Objetivo de precisiÃ³n: {target_accuracy:.1%}")
        print(f"   âœ… Tests ejecutados: {controller_results['tests_run']}")
        print(f"   ğŸ“ˆ Tasa de Ã©xito: {controller_results['success_rate']:.1%}")
        print(f"   â±ï¸ Tiempo ejecuciÃ³n: {controller_results['execution_time']:.2f}s")
        print(f"   ğŸ† Estado: {status}")
        
        if controller_results['failures'] > 0:
            print(f"   âš ï¸ Fallos: {controller_results['failures']}")
        
        if controller_results['errors'] > 0:
            print(f"   âŒ Errores: {controller_results['errors']}")
        
        return controller_results
    
    def run_all_tests(self, controllers: List[str] = None) -> Dict[str, Any]:
        """
        Ejecuta tests de accuracy para todos los controladores especificados
        
        Args:
            controllers: Lista de controladores a testear (None = todos)
            
        Returns:
            Dict con resultados completos
        """
        if controllers is None:
            controllers = list(self.available_tests.keys())
        
        print("\nğŸ¯ INICIANDO SUITE COMPLETA DE TESTS DE PRECISIÃ“N")
        print("=" * 80)
        
        self.test_start_time = time.time()
        all_results = {}
        
        # Ejecutar tests por controlador
        for controller in controllers:
            try:
                controller_results = self.run_controller_tests(controller)
                all_results[controller] = controller_results
                
                # Pausa entre controladores
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ Error ejecutando tests para {controller}: {e}")
                all_results[controller] = {
                    'controller': controller,
                    'error': str(e),
                    'status': 'âŒ ERROR'
                }
        
        self.test_end_time = time.time()
        
        # Generar reporte final
        self.generate_final_report(all_results)
        
        return all_results
    
    def generate_final_report(self, results: Dict[str, Any]):
        """
        Genera reporte final de todos los tests de accuracy
        
        Args:
            results: Resultados de todos los controladores
        """
        total_time = self.test_end_time - self.test_start_time
        
        print(f"\n{'='*80}")
        print("ğŸ“Š REPORTE FINAL DE PRECISIÃ“N DE GESTOS")
        print(f"{'='*80}")
        
        print(f"\nâ±ï¸ Tiempo total de ejecuciÃ³n: {total_time:.2f}s")
        print(f"ğŸ“… Ejecutado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # EstadÃ­sticas globales
        total_tests = sum(r.get('tests_run', 0) for r in results.values() if 'tests_run' in r)
        total_failures = sum(r.get('failures', 0) for r in results.values() if 'failures' in r)
        total_errors = sum(r.get('errors', 0) for r in results.values() if 'errors' in r)
        overall_success = (total_tests - total_failures - total_errors) / total_tests if total_tests > 0 else 0
        
        print(f"\nğŸ“ˆ ESTADÃSTICAS GLOBALES:")
        print(f"   ğŸ¯ Tests totales ejecutados: {total_tests}")
        print(f"   âœ… Tasa de Ã©xito global: {overall_success:.1%}")
        print(f"   âš ï¸ Fallos totales: {total_failures}")
        print(f"   âŒ Errores totales: {total_errors}")
        
        # Ranking de controladores por precisiÃ³n
        print(f"\nğŸ† RANKING DE PRECISIÃ“N POR CONTROLADOR:")
        
        valid_results = [(name, data) for name, data in results.items() if 'success_rate' in data]
        valid_results.sort(key=lambda x: x[1]['success_rate'], reverse=True)
        
        for i, (controller, data) in enumerate(valid_results, 1):
            target = self.accuracy_targets.get(controller, 0.85)
            actual = data['success_rate']
            target_met = "âœ…" if actual >= target * 0.90 else "âš ï¸"  # 90% del objetivo como umbral
            
            print(f"   {i}. {controller.upper():12} - {actual:.1%} (objetivo: {target:.1%}) {target_met}")
        
        # Controladores que necesitan atenciÃ³n
        needs_attention = [
            (name, data) for name, data in valid_results 
            if data['success_rate'] < self.accuracy_targets.get(name, 0.85) * 0.90
        ]
        
        if needs_attention:
            print(f"\nâš ï¸ CONTROLADORES QUE NECESITAN ATENCIÃ“N:")
            for controller, data in needs_attention:
                target = self.accuracy_targets.get(controller, 0.85)
                gap = target - data['success_rate']
                print(f"   â€¢ {controller.upper()}: {data['success_rate']:.1%} (brecha: -{gap:.1%})")
        
        # Recomendaciones
        print(f"\nğŸ’¡ RECOMENDACIONES:")
        
        if overall_success >= 0.90:
            print("   âœ… Excelente precisiÃ³n global. Sistema listo para producciÃ³n.")
        elif overall_success >= 0.80:
            print("   âœ… Buena precisiÃ³n global. Considerar mejoras menores.")
        elif overall_success >= 0.70:
            print("   âš ï¸ PrecisiÃ³n aceptable. Revisar controladores con bajo rendimiento.")
        else:
            print("   âŒ PrecisiÃ³n insuficiente. Requiere mejoras significativas.")
        
        if total_failures > 0:
            print(f"   ğŸ”§ Revisar {total_failures} tests que fallaron")
        
        if total_errors > 0:
            print(f"   ğŸ› Corregir {total_errors} errores encontrados")
        
        # Generar archivo de resultados
        self.save_results_to_file(results)
        
        print(f"\nğŸ’¾ Resultados guardados en: accuracy_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        print("=" * 80)
    
    def save_results_to_file(self, results: Dict[str, Any]):
        """
        Guarda los resultados en un archivo JSON
        
        Args:
            results: Resultados a guardar
        """
        try:
            # Preparar datos para JSON
            json_data = {
                'test_type': 'accuracy',
                'execution_date': datetime.now().isoformat(),
                'total_execution_time': self.test_end_time - self.test_start_time if self.test_start_time else 0,
                'controllers_tested': len(results),
                'accuracy_targets': self.accuracy_targets,
                'results': results
            }
            
            # Guardar archivo
            filename = f"accuracy_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = os.path.join(os.path.dirname(__file__), 'reports', filename)
            
            # Crear directorio si no existe
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âš ï¸ Error guardando resultados: {e}")


class TestResultCapture:
    """Captura de resultados de tests para procesamiento"""
    
    def __init__(self):
        self.output = []
    
    def write(self, text):
        self.output.append(text)
        # TambiÃ©n mostrar en consola
        print(text, end='')
    
    def flush(self):
        pass


def main():
    """FunciÃ³n principal"""
    runner = AccuracyTestRunner()
    
    # Argumentos de lÃ­nea de comandos simples
    if len(sys.argv) > 1:
        # Controladores especÃ­ficos
        controllers = sys.argv[1].split(',')
        print(f"ğŸ¯ Ejecutando tests de accuracy para: {', '.join(controllers)}")
        runner.run_all_tests(controllers)
    else:
        # Todos los controladores
        print("ğŸ¯ Ejecutando tests de accuracy para todos los controladores")
        runner.run_all_tests()


if __name__ == '__main__':
    main() 